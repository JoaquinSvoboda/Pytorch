{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 2.0 not found\n",
      "Requirement already satisfied: torchtext in /Users/joacogalt/anaconda3/lib/python3.9/site-packages (0.18.0)\n",
      "Requirement already satisfied: tqdm in /Users/joacogalt/anaconda3/lib/python3.9/site-packages (from torchtext) (4.67.1)\n",
      "Requirement already satisfied: requests in /Users/joacogalt/anaconda3/lib/python3.9/site-packages (from torchtext) (2.32.5)\n",
      "Requirement already satisfied: torch>=2.3.0 in /Users/joacogalt/anaconda3/lib/python3.9/site-packages (from torchtext) (2.3.0)\n",
      "Requirement already satisfied: numpy in /Users/joacogalt/anaconda3/lib/python3.9/site-packages (from torchtext) (1.26.4)\n",
      "Requirement already satisfied: filelock in /Users/joacogalt/anaconda3/lib/python3.9/site-packages (from torch>=2.3.0->torchtext) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/joacogalt/anaconda3/lib/python3.9/site-packages (from torch>=2.3.0->torchtext) (4.15.0)\n",
      "Requirement already satisfied: sympy in /Users/joacogalt/anaconda3/lib/python3.9/site-packages (from torch>=2.3.0->torchtext) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/joacogalt/anaconda3/lib/python3.9/site-packages (from torch>=2.3.0->torchtext) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /Users/joacogalt/anaconda3/lib/python3.9/site-packages (from torch>=2.3.0->torchtext) (2.11.3)\n",
      "Requirement already satisfied: fsspec in /Users/joacogalt/anaconda3/lib/python3.9/site-packages (from torch>=2.3.0->torchtext) (2025.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/joacogalt/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=2.3.0->torchtext) (2.0.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/joacogalt/anaconda3/lib/python3.9/site-packages (from requests->torchtext) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/joacogalt/anaconda3/lib/python3.9/site-packages (from requests->torchtext) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/joacogalt/anaconda3/lib/python3.9/site-packages (from requests->torchtext) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/joacogalt/anaconda3/lib/python3.9/site-packages (from requests->torchtext) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/joacogalt/anaconda3/lib/python3.9/site-packages (from sympy->torch>=2.3.0->torchtext) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install portalocker>=2.0\n",
    "! pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0 0.18.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "print(torch.__version__, torchtext.__version__)\n",
    "from torchtext.datasets import DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'E. D. Abbott Ltd', ' Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.')\n"
     ]
    }
   ],
   "source": [
    "from dbpedia import DBpedia\n",
    "\n",
    "train_iter = iter(DBpedia(split=\"train\"))\n",
    "print(next(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizador = get_tokenizer(\"basic_english\")\n",
    "train_iter = DBpedia(split=\"train\")\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text, description in data_iter:\n",
    "        yield tokenizador(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2673, 629, 480, 97, 0, 40, 812, 17, 0, 2421]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(tokenizador(\"hello how are you? i am a platzi student\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_pipeline = lambda x: vocab(tokenizador(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2673, 40, 812, 7907]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_pipeline(\"hello i am joaquin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline(\"10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuración del dispositivo: Determina si se usará CPU o GPU (CUDA) según la disponibilidad.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "#Definición de collateBatch: Desarrollar una función collateBatch para procesar cada lote.\n",
    "def collateBatch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(texto_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0) + offsets[-1])\n",
    "    \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64).to(device)\n",
    "    #La función cumsum en PyTorch proporciona la suma acumulativa de los elementos a lo largo de una dimensión especificada. En este contexto, se utiliza para determinar los puntos de inicio de cada nuevo texto dentro del tensor de datos concatenados. Este método es vital para gestionar adecuadamente el flujo de datos en estructuras de texto:\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0).to(device)\n",
    "    text_list = torch.cat(text_list).to(device)\n",
    "    return label_list, text_list, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "trainIter = DBpedia(split='train') #Definición del dataset: Se utiliza un dataset de entrenamiento, en este caso un iterador llamado trainIter del dataset DBpedia, especificando su uso para el set de entrenamiento:\n",
    "data_loader = DataLoader(dataset=trainIter, batch_size=8, shuffle=False, collate_fn=collateBatch) #Creación del Data Loader: Aquí se establece el tamaño del lote, se elige si se quiere aleatorizar la secuencia de los datos (shuffling) y se selecciona una función collate:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x38cd44a60>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModeloClasificacionTexto(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(ModeloClasificacionTexto, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.bn1 = nn.BatchNorm1d(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        normEmbedded = self.bn1(embedded)\n",
    "        embeddedActivated = F.relu(normEmbedded)\n",
    "        return self.fc(embeddedActivated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = DBpedia(split=\"train\")\n",
    "num_class = len(set([item[1] for item in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 100\n",
    "modelo = ModeloClasificacionTexto(vocab_size=vocab_size, embed_dim=embedding_size, num_classes=num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313710"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo tiene 87,931,200 parámetros entrenables\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "modelo = ModeloClasificacionTexto(vocab_size=vocab_size, embed_dim=embedding_size, num_classes=num_class)\n",
    "print(f\"El modelo tiene {count_parameters(modelo):,} parámetros entrenables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrena(dataloader):\n",
    "    # Colocar el modelo en formato de entrenamiento\n",
    "    modelo.train()\n",
    "\n",
    "    # Inicializa accuracy, count y loss para cada epoch\n",
    "    epoch_acc = 0\n",
    "    epoch_loss = 0\n",
    "    total_count = 0 \n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        # reestablece los gradientes después de cada batch\n",
    "        optimizer.zero_grad()\n",
    "        # Obten predicciones del modelo\n",
    "        prediccion = modelo(text, offsets)\n",
    "\n",
    "        # Obten la pérdida\n",
    "        loss = criterio(prediccion, label)\n",
    "        \n",
    "        # backpropage la pérdida y calcular los gradientes\n",
    "        loss.backward()\n",
    "        \n",
    "        # Obten la accuracy\n",
    "        acc = (prediccion.argmax(1) == label).sum()\n",
    "        \n",
    "        # Evita que los gradientes sean demasiado grandes \n",
    "        torch.nn.utils.clip_grad_norm_(modelo.parameters(), 0.1, foreach=False)\n",
    "\n",
    "        # Actualiza los pesos\n",
    "        optimizer.step()\n",
    "\n",
    "        # Llevamos el conteo de la pérdida y el accuracy para esta epoch\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_loss += loss.item()\n",
    "        total_count += label.size(0)\n",
    "\n",
    "        if idx % 500 == 0 and idx > 0:\n",
    "          print(f\" epoca {epoch} | {idx}/{len(dataloader)} batches | perdida {epoch_loss/total_count} | accuracy {epoch_acc/total_count}\")\n",
    "\n",
    "    return epoch_acc/total_count, epoch_loss/total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalua(dataloader):\n",
    "  modelo.eval()\n",
    "  epoch_acc = 0\n",
    "  total_count = 0\n",
    "  epoch_loss = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            # Obtenemos la la etiqueta predecida\n",
    "      prediccion = modelo(text, offsets)\n",
    "\n",
    "            # Obtenemos pérdida y accuracy\n",
    "      loss = criterio(prediccion, label)\n",
    "      acc = (prediccion.argmax(1) == label).sum()\n",
    "            \n",
    "            # Llevamos el conteo de la pérdida y el accuracy para esta epoch\n",
    "      epoch_loss += loss.item()\n",
    "      epoch_acc += acc.item()\n",
    "      total_count += label.size(0)\n",
    "\n",
    "  return epoch_acc/total_count, epoch_loss/total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros\n",
    "\n",
    "EPOCHS = 4 # epochs\n",
    "TASA_APRENDIZAJE = 0.2  # tasa de aprendizaje\n",
    "BATCH_TAMANO = 64 # tamaño de los batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pérdida, optimizador\n",
    "criterio = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(modelo.parameters(), lr= TASA_APRENDIZAJE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    labels = []\n",
    "    texts = []\n",
    "\n",
    "    for label, title, text in batch:\n",
    "        full_text = (title + \" \" + text).lower()\n",
    "        token_ids = vocab(tokenizador(full_text))\n",
    "        labels.append(label - 1)\n",
    "        texts.append(torch.tensor(token_ids, dtype=torch.int64))\n",
    "\n",
    "    labels = torch.tensor(labels, dtype=torch.int64)\n",
    "    offsets = torch.tensor([0] + [len(t) for t in texts[:-1]]).cumsum(0)\n",
    "    texts = torch.cat(texts)\n",
    "\n",
    "    return labels.to(device), texts.to(device), offsets.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "train_iter = list(DBpedia(\"train\"))\n",
    "test_iter = list(DBpedia(\"test\"))\n",
    "\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_TAMANO, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_TAMANO, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_TAMANO, shuffle=True, collate_fn=collate_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'E. D. Abbott Ltd', ' Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.')\n"
     ]
    }
   ],
   "source": [
    "ejemplo = next(iter(DBpedia(split=\"train\")))\n",
    "print(ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obten la mejor pérdida \n",
    "major_loss_validation = float('inf')\n",
    "\n",
    "# Entrenamos\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Entrenamiento\n",
    "    entrenamiento_acc, entrenamiento_loss = entrena(train_dataloader)\n",
    "    \n",
    "    # Validación\n",
    "    validacion_acc, validacion_loss = evalua(valid_dataloader)\n",
    "\n",
    "    # Guarda el mejor modelo\n",
    "    if validacion_loss < major_loss_validation:\n",
    "      best_valid_loss = validacion_loss\n",
    "      torch.save(modelo.state_dict(), \"mejores_guardados.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_loss = evalua(test_dataloader)\n",
    "\n",
    "print(f'Accuracy del test dataset -> {test_acc}')\n",
    "print(f'Pérdida del test dataset -> {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBpedia_label = {1: 'Company',\n",
    "                2: 'EducationalInstitution',\n",
    "                3: 'Artist',\n",
    "                4: 'Athlete',\n",
    "                5: 'OfficeHolder',\n",
    "                6: 'MeanOfTransportation',\n",
    "                7: 'Building',\n",
    "                8: 'NaturalPlace',\n",
    "                9: 'Village',\n",
    "                10: 'Animal',\n",
    "                11: 'Plant',\n",
    "                12: 'Album',\n",
    "                13: 'Film',\n",
    "                14: 'WrittenWork'}\n",
    "\n",
    "def predict(text, texto_pipeline):\n",
    "  with torch.no_grad():\n",
    "    text = torch.tensor(texto_pipeline(text))\n",
    "    opt_mod = torch.compile(model, mode=\"reduce-overhead\")\n",
    "    output = opt_mod(text, torch.tensor([0]))\n",
    "    return output.argmax(1).item() + 1\n",
    "\n",
    "\n",
    "ejemplo_1 = \"Nithari is a village in the western part of the state of Uttar Pradesh India bordering on New Delhi. Nithari forms part of the New Okhla Industrial Development Authority's planned industrial city Noida falling in Sector 31. Nithari made international news headlines in December 2006 when the skeletons of a number of apparently murdered women and children were unearthed in the village.\"\n",
    "\n",
    "\n",
    "model = modelo.to(\"cpu\")\n",
    "\n",
    "\n",
    "print(f\"El ejemplo 1 es de categoría {DBpedia_label[predict(ejemplo_1, texto_pipeline)]}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "260cca62e720824a4620f75acf6c813760a2eb1c1e272aa6eb4d532c286c0702"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
